{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten,BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.constraints import maxnorm\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from random import seed\n",
    "from random import randint\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report as cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emocion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>uso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emotion</td>\n",
       "      <td>pixels</td>\n",
       "      <td>Usage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>55 55 55 55 55 54 60 68 54 85 151 163 170 179 ...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>20 17 19 21 25 38 42 42 46 54 56 62 63 66 82 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>77 78 79 79 78 75 60 55 47 48 58 73 77 79 57 5...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>85 84 90 121 101 102 133 153 153 169 177 189 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emocion                                             pixels       uso\n",
       "0  emotion                                             pixels     Usage\n",
       "1        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "2        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "3        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "4        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "5        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n",
       "6        2  55 55 55 55 55 54 60 68 54 85 151 163 170 179 ...  Training\n",
       "7        4  20 17 19 21 25 38 42 42 46 54 56 62 63 66 82 1...  Training\n",
       "8        3  77 78 79 79 78 75 60 55 47 48 58 73 77 79 57 5...  Training\n",
       "9        3  85 84 90 121 101 102 133 153 153 169 177 189 1...  Training"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 16\n",
    "filename = '../Datos/fer2013.csv'\n",
    "label_map = ['Enfado', 'Asco', 'Miedo', 'Alegria', 'Tristeza', 'Sorpresa', 'Neutral']\n",
    "label_map = np.array(label_map)\n",
    "names=['emocion','pixels','uso']\n",
    "df=pd.read_csv(filename,names=names, na_filter=False)\n",
    "im=df['pixels']\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(filname):\n",
    "    \n",
    "    Y_train = []\n",
    "    X_train = []\n",
    "    Y_test = []\n",
    "    X_test = []\n",
    "    \n",
    "    first = True\n",
    "    uso = 'Training\\n'\n",
    "    for line in open(filname):\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            row = line.split(',')\n",
    "            condition = str(row[2])\n",
    "            if condition == 'Training\\n':\n",
    "                Y_train.append(int(row[0]))\n",
    "                X_train.append([int(p) for p in row[1].split()])\n",
    "            else:\n",
    "                Y_test.append(int(row[0]))\n",
    "                X_test.append([int(p) for p in row[1].split()])\n",
    "                \n",
    "    X_train, Y_train = np.array(X_train)/255.0 , np.array(Y_train)\n",
    "    X_test, Y_test = np.array(X_test)/255.0 , np.array(Y_test)\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test= loadData(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = X_train.shape[0]\n",
    "N_test = X_test.shape[0]\n",
    "X_train = X_train.reshape(N_train, 48, 48,1)\n",
    "X_test = X_test.reshape(N_test, 48, 48,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode output\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_test = np_utils.to_categorical(Y_test)\n",
    "class_num = Y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 96)        11712     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 24, 24, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 256)       614656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 384)       885120    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 236)       815852    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 33984)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              139202560 \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 28679     \n",
      "=================================================================\n",
      "Total params: 159,667,379\n",
      "Trainable params: 159,667,379\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(96,(11,11),input_shape=X_train.shape[1:],padding = 'same', strides = 1, activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(256,(5,5),input_shape=X_train.shape[1:],padding = 'same', strides = 1, activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(384,(3,3),input_shape=X_train.shape[1:],padding = 'same', strides = 1, activation = 'relu'))\n",
    "model.add(Conv2D(384,(3,3),input_shape=X_train.shape[1:],padding = 'same', strides = 1, activation = 'relu'))\n",
    "model.add(Conv2D(236,(3,3),input_shape=X_train.shape[1:],padding = 'same', strides = 1, activation = 'relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dense(class_num, activation='softmax' ))\n",
    "model.summary()\n",
    "\n",
    "optimizer = 'adam'\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "449/449 [==============================] - 1845s 4s/step - loss: 1.8430 - accuracy: 0.2496 - val_loss: 1.8193 - val_accuracy: 0.2471 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "449/449 [==============================] - 1780s 4s/step - loss: 1.8126 - accuracy: 0.2513 - val_loss: 1.8141 - val_accuracy: 0.2471 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "449/449 [==============================] - 1726s 4s/step - loss: 1.8115 - accuracy: 0.2513 - val_loss: 1.8148 - val_accuracy: 0.2471 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "449/449 [==============================] - 1743s 4s/step - loss: 1.8108 - accuracy: 0.2513 - val_loss: 1.8133 - val_accuracy: 0.2471 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "449/449 [==============================] - 1733s 4s/step - loss: 1.8110 - accuracy: 0.2513 - val_loss: 1.8134 - val_accuracy: 0.2471 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "449/449 [==============================] - 1757s 4s/step - loss: 1.8108 - accuracy: 0.2513 - val_loss: 1.8132 - val_accuracy: 0.2471 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "449/449 [==============================] - 1754s 4s/step - loss: 1.8111 - accuracy: 0.2513 - val_loss: 1.8134 - val_accuracy: 0.2471 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "449/449 [==============================] - 1739s 4s/step - loss: 1.8109 - accuracy: 0.2513 - val_loss: 1.8140 - val_accuracy: 0.2471 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "449/449 [==============================] - 1725s 4s/step - loss: 1.8108 - accuracy: 0.2513 - val_loss: 1.8149 - val_accuracy: 0.2471 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "449/449 [==============================] - 1714s 4s/step - loss: 1.8108 - accuracy: 0.2513 - val_loss: 1.8138 - val_accuracy: 0.2471 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "449/449 [==============================] - 1696s 4s/step - loss: 1.8112 - accuracy: 0.2513 - val_loss: 1.8133 - val_accuracy: 0.2471 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "449/449 [==============================] - 1700s 4s/step - loss: 1.8107 - accuracy: 0.2513 - val_loss: 1.8146 - val_accuracy: 0.2471 - lr: 0.0010\n",
      "Epoch 13/100\n",
      " 47/449 [==>...........................] - ETA: 34:36:08 - loss: 1.8080 - accuracy: 0.2606"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mc = ModelCheckpoint('.anet_1.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, mode='min')\n",
    "\n",
    "np.random.seed(17)\n",
    "cb =  [es,mc,rlr]\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=epochs, batch_size=64, callbacks = cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (5,5))\n",
    "  \n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    ax.set_ylim(-0.5, 6.5)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('.anet_1.hdf5')\n",
    "pred = np.array(model.predict(X_test))\n",
    "pred = np.around(pred)\n",
    "pred = pred.astype(int)\n",
    "Y_test = Y_test.astype(int)\n",
    "\n",
    "\n",
    "plot_confusion_matrix(Y_test.argmax(axis=1),pred.argmax(axis=1),label_map)\n",
    "print(cr(Y_test.argmax(axis=1),pred.argmax(axis=1),target_names = label_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 48, 48, 96)        11712     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 48, 48, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 24, 24, 96)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 24, 24, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 24, 24, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 24, 24, 256)       614656    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 24, 24, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 12, 384)       885120    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 12, 12, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12, 12, 384)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 12, 12, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 12, 12, 236)       815852    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 12, 12, 236)       0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 12, 12, 236)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 12, 12, 236)       944       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 33984)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4096)              139202560 \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 7)                 28679     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 159,705,571\n",
      "Trainable params: 159,686,475\n",
      "Non-trainable params: 19,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(96,(11,11),input_shape=X_train.shape[1:],padding = 'same', strides = 1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(256,(5,5),input_shape=X_train.shape[1:],padding = 'same', strides = 1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(384,(3,3),input_shape=X_train.shape[1:],padding = 'same', strides = 1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(384,(3,3),input_shape=X_train.shape[1:],padding = 'same', strides = 1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(256,(3,3),input_shape=X_train.shape[1:],padding = 'same', strides = 1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, kernel_constraint=maxnorm(3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(4096, kernel_constraint=maxnorm(3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(class_num, kernel_constraint=maxnorm(3)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "optimizer = 'adam'\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "449/449 [==============================] - 2423s 5s/step - loss: 2.2557 - accuracy: 0.2546 - val_loss: 2.0182 - val_accuracy: 0.2888\n",
      "Epoch 2/100\n",
      "449/449 [==============================] - 57543s 128s/step - loss: 1.8626 - accuracy: 0.3257 - val_loss: 1.8756 - val_accuracy: 0.2689\n",
      "Epoch 3/100\n",
      "449/449 [==============================] - 2557s 6s/step - loss: 1.7680 - accuracy: 0.3679 - val_loss: 3.6769 - val_accuracy: 0.3544\n",
      "Epoch 4/100\n",
      "449/449 [==============================] - 2553s 6s/step - loss: 1.7047 - accuracy: 0.3948 - val_loss: 1.6426 - val_accuracy: 0.4245\n",
      "Epoch 5/100\n",
      "449/449 [==============================] - 2530s 6s/step - loss: 1.6494 - accuracy: 0.4183 - val_loss: 1.5247 - val_accuracy: 0.4432\n",
      "Epoch 6/100\n",
      "449/449 [==============================] - 2505s 6s/step - loss: 1.6223 - accuracy: 0.4302 - val_loss: 1.6379 - val_accuracy: 0.3805\n",
      "Epoch 7/100\n",
      "449/449 [==============================] - 2500s 6s/step - loss: 1.5959 - accuracy: 0.4441 - val_loss: 1.8690 - val_accuracy: 0.4323\n",
      "Epoch 8/100\n",
      "449/449 [==============================] - 2490s 6s/step - loss: 1.4973 - accuracy: 0.4717 - val_loss: 1.5487 - val_accuracy: 0.4019\n",
      "Epoch 9/100\n",
      "449/449 [==============================] - 2510s 6s/step - loss: 1.3677 - accuracy: 0.4864 - val_loss: 1.4287 - val_accuracy: 0.4570\n",
      "Epoch 10/100\n",
      "449/449 [==============================] - 2505s 6s/step - loss: 1.3039 - accuracy: 0.5092 - val_loss: 1.3200 - val_accuracy: 0.5079\n",
      "Epoch 11/100\n",
      "449/449 [==============================] - 2472s 6s/step - loss: 1.5063 - accuracy: 0.4687 - val_loss: 1.6445 - val_accuracy: 0.3667\n",
      "Epoch 12/100\n",
      "449/449 [==============================] - 2497s 6s/step - loss: 1.5037 - accuracy: 0.4522 - val_loss: 1.3427 - val_accuracy: 0.4802\n",
      "Epoch 13/100\n",
      "449/449 [==============================] - 2478s 6s/step - loss: 1.3197 - accuracy: 0.5047 - val_loss: 1.2966 - val_accuracy: 0.5233\n",
      "Epoch 14/100\n",
      "449/449 [==============================] - 2446s 5s/step - loss: 1.2577 - accuracy: 0.5304 - val_loss: 2.5598 - val_accuracy: 0.4763\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20910448488>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc = ModelCheckpoint('.anet_2.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "cb =  [es,mc,rlr]\n",
    "\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=epochs, batch_size=64, callbacks = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('.anet_2.hdf5')\n",
    "pred = np.array(model.predict(X_test))\n",
    "pred = np.around(pred)\n",
    "pred = pred.astype(int)\n",
    "Y_test = Y_test.astype(int)\n",
    "\n",
    "\n",
    "plot_confusion_matrix(Y_test.argmax(axis=1),pred.argmax(axis=1),label_map)\n",
    "print(cr(Y_test.argmax(axis=1),pred.argmax(axis=1),target_names = label_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test= loadData(filename)\n",
    "X_train = X_train.reshape(N_train, 48, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mirror(img):\n",
    "    copy = img.copy()\n",
    "    indx = range(copy.shape[1]-1, -1, -1)\n",
    "    copy = np.flip(copy, axis = 1)\n",
    "    return copy\n",
    "\n",
    "\n",
    "def random_erasing(img):\n",
    "    copy = img.copy()\n",
    "    numero_parches=(randint(1,3))\n",
    "    for i in range(numero_parches):\n",
    "        tam_h = randint(5,15)\n",
    "        tam_v = randint(5,15)   \n",
    "        start_h = randint(0,copy.shape[1]-tam_h)\n",
    "        start_v = randint(0,copy.shape[0]-tam_v) \n",
    "        for i in range(tam_v):\n",
    "            for j in range(tam_h):\n",
    "                copy[start_v+i, start_h+j] = 1\n",
    "    return copy\n",
    "\n",
    "\n",
    "def tras_x(img, tras):\n",
    "    order = []\n",
    "    for i in range(tras):\n",
    "        order = order + [0]\n",
    "    order = order + list(range(img.shape[1]- tras))\n",
    "    copy = img[:,order]\n",
    "    return copy\n",
    "\n",
    "\n",
    "def tras_y(img, tras):\n",
    "    order = []\n",
    "    for i in range(tras):\n",
    "        order = order + [0]\n",
    "    order = order + list(range(img.shape[0]- tras))\n",
    "    copy = img[order,:]\n",
    "    return copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo = []\n",
    "etiquetas = []\n",
    "\n",
    "\n",
    "for i in range(N_train):\n",
    "    \n",
    "    if np.random.random() < 0.5:\n",
    "        n = mirror(X_train[i])\n",
    "        nuevo += [n]\n",
    "        etiquetas += [Y_train[i]]\n",
    "        \n",
    "    if np.random.random() < 0.5:\n",
    "        if np.random.random() < 0.5:\n",
    "            n = tras_x(X_train[i],randint(5,15))\n",
    "        else:\n",
    "            n = tras_y(X_train[i],randint(5,10))\n",
    "        nuevo += [n]\n",
    "        etiquetas += [Y_train[i]]\n",
    "        \n",
    "    if np.random.random() < 0.25:\n",
    "        n = random_erasing(X_train[i])\n",
    "        nuevo += [n]\n",
    "        etiquetas += [Y_train[i]]\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "if len(etiquetas) > 0: \n",
    "    nuevo = np.array(nuevo)\n",
    "    etiquetas =  np.array(etiquetas)\n",
    "    X_train= np.append(X_train,nuevo,axis=0)\n",
    "    Y_train = np.append(Y_train, etiquetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = X_train.shape[0]\n",
    "N_test = X_test.shape[0]\n",
    "X_train = X_train.reshape(N_train, 48, 48,1)\n",
    "X_test = X_test.reshape(N_test, 48, 48,1)\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_test = np_utils.to_categorical(Y_test)\n",
    "class_num = Y_test.shape[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
